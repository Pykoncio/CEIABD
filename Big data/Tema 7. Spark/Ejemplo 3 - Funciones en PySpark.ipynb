{"cells":[{"cell_type":"markdown","metadata":{"id":"0A2CmXykymnl"},"source":["# Funciones en PySpark"]},{"cell_type":"markdown","metadata":{"id":"-WFYGrlTymnn"},"source":["En este notebook aprenderemos algunas funciones avanzadas para optimizar el rendimiento de Spark, para imputar valores faltantes o a crear funciones definidas por el usuario (UDF)."]},{"cell_type":"code","source":["# Install spark-related dependencies\n","!wget -q  https://apache.osuosl.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n","!tar xf spark-3.5.0-bin-hadoop3.tgz\n","\n","!pip install -q findspark\n","!pip install pyspark\n","# Set up required environment variables\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pAvJUSeiy1-G","executionInfo":{"status":"ok","timestamp":1708282526250,"user_tz":-60,"elapsed":56070,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}},"outputId":"3070eaee-d9e1-482b-fa8b-beebe87e765f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=cb9677a556b6799a6d4be25cea3df3a539f8b54e0f402e2ac40cf54467ff181b\n","  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.0\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"guI6zKboymnn","executionInfo":{"status":"ok","timestamp":1708282526251,"user_tz":-60,"elapsed":6,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}}},"outputs":[],"source":["import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"AJHnAYm4ymno","executionInfo":{"status":"ok","timestamp":1708282526463,"user_tz":-60,"elapsed":217,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.functions import broadcast\n","from pyspark.sql.types import *"]},{"cell_type":"markdown","metadata":{"id":"1AbOLspYymno"},"source":["### Crea la sesión de SparkSession"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Na7Qb23fymno","executionInfo":{"status":"ok","timestamp":1708282535883,"user_tz":-60,"elapsed":9422,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}}},"outputs":[],"source":["spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"bEEYc6vvymno"},"source":["### Crear el DataFrame"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"CXoss716ymno","executionInfo":{"status":"ok","timestamp":1708282541393,"user_tz":-60,"elapsed":5516,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}}},"outputs":[],"source":["emp = [(1, \"AAA\", \"dept1\", 1000),\n","    (2, \"BBB\", \"dept1\", 1100),\n","    (3, \"CCC\", \"dept1\", 3000),\n","    (4, \"DDD\", \"dept1\", 5500),\n","    (5, \"EEE\", \"dept2\", 8000),\n","    (6, \"FFF\", \"dept2\", 9200),\n","    (7, \"GGG\", \"dept3\", 1100),\n","    (None, None, None, 5500),\n","    (9, \"III\", None, 3500),\n","    (10, None, \"dept5\", 2500)]\n","\n","dept = [(\"dept1\", \"Department - 1\"),\n","        (\"dept2\", \"Department - 2\"),\n","        (\"dept3\", \"Department - 3\"),\n","        (\"dept4\", \"Department - 4\")\n","       ]\n","\n","df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n","deptdf = spark.createDataFrame(dept, [\"id\", \"name\"])\n","\n","# Create Temp Tables\n","df.createOrReplaceTempView(\"empdf\")\n","deptdf.createOrReplaceTempView(\"deptdf\")"]},{"cell_type":"markdown","metadata":{"id":"VuD-OiRGymno"},"source":["#  Expresiones SQL"]},{"cell_type":"markdown","metadata":{"id":"ZpcC8N4Tymno"},"source":["También podemos usar la expresión SQL para la manipulación de datos. Tenemos la función **expr** y también una variante de un método de selección como **selectExpr** para la evaluación de expresiones SQL."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tICFpNm8ymnp","executionInfo":{"status":"ok","timestamp":1708282548243,"user_tz":-60,"elapsed":6854,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}},"outputId":"1db966e7-ae95-4bfb-f946-e5dccef2209a"},"outputs":[{"output_type":"stream","name":"stdout","text":["+----+----+-----+------+------------+\n","|  id|name| dept|salary|salary_level|\n","+----+----+-----+------+------------+\n","|   1| AAA|dept1|  1000|  low_salary|\n","|   2| BBB|dept1|  1100|  low_salary|\n","|   3| CCC|dept1|  3000|  mid_salary|\n","|   4| DDD|dept1|  5500| high_salary|\n","|   5| EEE|dept2|  8000| high_salary|\n","|   6| FFF|dept2|  9200| high_salary|\n","|   7| GGG|dept3|  1100|  low_salary|\n","|NULL|NULL| NULL|  5500| high_salary|\n","|   9| III| NULL|  3500|  mid_salary|\n","|  10|NULL|dept5|  2500|  mid_salary|\n","+----+----+-----+------+------------+\n","\n"]}],"source":["from pyspark.sql.functions import expr\n","\n","cond = \"\"\"case when salary > 5000 then 'high_salary'\n","               else case when salary > 2000 then 'mid_salary'\n","                    else case when salary > 0 then 'low_salary'\n","                         else 'invalid_salary'\n","                              end\n","                         end\n","                end as salary_level\"\"\"\n","\n","newdf = df.withColumn(\"salary_level\", expr(cond))\n","newdf.show()"]},{"cell_type":"markdown","metadata":{"id":"EMQvVvZaymnp"},"source":["### Usando la función selectExpr"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"CrVd7qppymnp","outputId":"39ff80b5-b56b-4de4-98a3-ac38224222a9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708282549320,"user_tz":-60,"elapsed":1082,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["+----+----+-----+------+------------+\n","|  id|name| dept|salary|salary_level|\n","+----+----+-----+------+------------+\n","|   1| AAA|dept1|  1000|  low_salary|\n","|   2| BBB|dept1|  1100|  low_salary|\n","|   3| CCC|dept1|  3000|  mid_salary|\n","|   4| DDD|dept1|  5500| high_salary|\n","|   5| EEE|dept2|  8000| high_salary|\n","|   6| FFF|dept2|  9200| high_salary|\n","|   7| GGG|dept3|  1100|  low_salary|\n","|NULL|NULL| NULL|  5500| high_salary|\n","|   9| III| NULL|  3500|  mid_salary|\n","|  10|NULL|dept5|  2500|  mid_salary|\n","+----+----+-----+------+------------+\n","\n"]}],"source":["newdf = df.selectExpr(\"*\", cond)\n","newdf.show()"]},{"cell_type":"markdown","metadata":{"id":"sdBm3XtYymnq"},"source":["### Funciones definidas por el usuario (UDF)\n","A menudo necesitamos escribir la función en función de nuestro requisito muy específico. Aquí podemos aprovechar las udfs. Podemos escribir nuestras propias funciones en un lenguaje como python y registrar la función como udf, luego podemos usar la función para operaciones de DataFrame.\n","\n","* Función de Python para encontrar el nivel_salario para un salario dado."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"AwZquHrPymnq","executionInfo":{"status":"ok","timestamp":1708282549320,"user_tz":-60,"elapsed":6,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}}},"outputs":[],"source":["def detSalary_Level(sal):\n","    level = None\n","\n","    if(sal > 5000):\n","        level = 'high_salary'\n","    elif(sal > 2000):\n","        level = 'mid_salary'\n","    elif(sal > 0):\n","        level = 'low_salary'\n","    else:\n","        level = 'invalid_salary'\n","    return level"]},{"cell_type":"markdown","metadata":{"id":"q29JJzxMymnq"},"source":["* Luego registre la función \"detSalary_Level\" como UDF."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"aurtlp89ymnq","executionInfo":{"status":"ok","timestamp":1708282549321,"user_tz":-60,"elapsed":6,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}}},"outputs":[],"source":["sal_level = udf(detSalary_Level, StringType())"]},{"cell_type":"markdown","metadata":{"id":"MsLdoEOBymnq"},"source":["* Aplicar función para determinar el salario_level para un salario dado."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfZ5Szf8ymnq","executionInfo":{"status":"ok","timestamp":1708282550991,"user_tz":-60,"elapsed":1675,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}},"outputId":"1d941a2f-3a63-4c86-e111-1b7e4f8d0102"},"outputs":[{"output_type":"stream","name":"stdout","text":["+----+----+-----+------+------------+\n","|  id|name| dept|salary|salary_level|\n","+----+----+-----+------+------------+\n","|   1| AAA|dept1|  1000|  low_salary|\n","|   2| BBB|dept1|  1100|  low_salary|\n","|   3| CCC|dept1|  3000|  mid_salary|\n","|   4| DDD|dept1|  5500| high_salary|\n","|   5| EEE|dept2|  8000| high_salary|\n","|   6| FFF|dept2|  9200| high_salary|\n","|   7| GGG|dept3|  1100|  low_salary|\n","|NULL|NULL| NULL|  5500| high_salary|\n","|   9| III| NULL|  3500|  mid_salary|\n","|  10|NULL|dept5|  2500|  mid_salary|\n","+----+----+-----+------+------------+\n","\n"]}],"source":["newdf = df.withColumn(\"salary_level\", sal_level(\"salary\"))\n","newdf.show()"]},{"cell_type":"markdown","source":["# Trabajando con valores NULL\n","\n","Los valores NULL siempre son difíciles de manejar independientemente del Framework o lenguaje que usemos. Aquí en Spark tenemos pocas funciones específicas para lidiar con valores NULL.\n","\n","**isNull**()\n","\n","Esta función nos ayudará a encontrar los valores nulos para cualquier columna dada. Por ejemplo si necesitamos encontrar las columnas donde las columnas id contienen los valores nulos."],"metadata":{"id":"9NM1Qa0J1-OZ"}},{"cell_type":"code","source":["newdf = df.filter(df[\"dept\"].isNull())\n","newdf.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PGzeGSii2Vjk","executionInfo":{"status":"ok","timestamp":1708282551581,"user_tz":-60,"elapsed":594,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}},"outputId":"f085761c-035a-4c6e-e2cc-4efa30de9975"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+----+----+------+\n","|  id|name|dept|salary|\n","+----+----+----+------+\n","|NULL|NULL|NULL|  5500|\n","|   9| III|NULL|  3500|\n","+----+----+----+------+\n","\n"]}]},{"cell_type":"markdown","source":["**isNotNull**()\n","\n","\n","Esta función funciona de manera opuesta a la función isNull () y devolverá todos los valores no nulos para una función en particular."],"metadata":{"id":"cI8btU692YD_"}},{"cell_type":"code","source":["newdf = df.filter(df[\"dept\"].isNotNull())\n","newdf.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cue3oKgL2k3A","executionInfo":{"status":"ok","timestamp":1708282552160,"user_tz":-60,"elapsed":582,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}},"outputId":"93fb4299-af7c-4e3b-9846-df5e766709ec"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+----+-----+------+\n","| id|name| dept|salary|\n","+---+----+-----+------+\n","|  1| AAA|dept1|  1000|\n","|  2| BBB|dept1|  1100|\n","|  3| CCC|dept1|  3000|\n","|  4| DDD|dept1|  5500|\n","|  5| EEE|dept2|  8000|\n","|  6| FFF|dept2|  9200|\n","|  7| GGG|dept3|  1100|\n","| 10|NULL|dept5|  2500|\n","+---+----+-----+------+\n","\n"]}]},{"cell_type":"markdown","source":["**fillna**()\n","\n","Esta función nos ayudará a reemplazar los valores nulos."],"metadata":{"id":"72Zzdu1a2swc"}},{"cell_type":"code","source":["# Replace -1 where the salary is null.\n","newdf = df.fillna(\"INVALID\", [\"dept\"])\n","newdf.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h3237g1T2wDI","executionInfo":{"status":"ok","timestamp":1708282552723,"user_tz":-60,"elapsed":565,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}},"outputId":"34d27fbf-6511-4454-d55e-258971120c95"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+----+-------+------+\n","|  id|name|   dept|salary|\n","+----+----+-------+------+\n","|   1| AAA|  dept1|  1000|\n","|   2| BBB|  dept1|  1100|\n","|   3| CCC|  dept1|  3000|\n","|   4| DDD|  dept1|  5500|\n","|   5| EEE|  dept2|  8000|\n","|   6| FFF|  dept2|  9200|\n","|   7| GGG|  dept3|  1100|\n","|NULL|NULL|INVALID|  5500|\n","|   9| III|INVALID|  3500|\n","|  10|NULL|  dept5|  2500|\n","+----+----+-------+------+\n","\n"]}]},{"cell_type":"markdown","source":["**dropna**()\n","\n","Esta función nos ayudará a eliminar las filas con valores nulos."],"metadata":{"id":"QXFik0ni2zAK"}},{"cell_type":"code","source":["newdf = df.dropna()\n","newdf.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hBQ1GHxl23ra","executionInfo":{"status":"ok","timestamp":1708282553327,"user_tz":-60,"elapsed":604,"user":{"displayName":"Miguel Ronda","userId":"13437494457168354933"}},"outputId":"5a0bec21-5780-4df3-9999-5379b7788381"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+----+-----+------+\n","| id|name| dept|salary|\n","+---+----+-----+------+\n","|  1| AAA|dept1|  1000|\n","|  2| BBB|dept1|  1100|\n","|  3| CCC|dept1|  3000|\n","|  4| DDD|dept1|  5500|\n","|  5| EEE|dept2|  8000|\n","|  6| FFF|dept2|  9200|\n","|  7| GGG|dept3|  1100|\n","+---+----+-----+------+\n","\n"]}]},{"cell_type":"code","source":["from random import randint\n","\n","# create a list of random numbers between 10 to 1000\n","my_large_list = [randint(10,1000) for x in range(0,20000000)]\n","\n","# create one partition of the list\n","my_large_list_one_partition = spark.parallelize(my_large_list,numSlices=1)\n","\n","# check number of partitions\n","print(my_large_list_one_partition.getNumPartitions())\n","# >> 1\n","\n","# filter numbers greater than equal to 200\n","my_large_list_one_partition = my_large_list_one_partition.filter(lambda x : x >= 200)\n","\n","# code was run in a jupyter notebook\n","# to calculate the time taken to execute the following command\n","%%time\n","\n","# count the number of elements in filtered list\n","print(my_large_list_one_partition.count())\n","# >> 16162207"],"metadata":{"id":"tHlvogr9kyE4"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}