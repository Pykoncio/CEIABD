{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A2CmXykymnl"
      },
      "source": [
        "# Funciones en PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WFYGrlTymnn"
      },
      "source": [
        "En este notebook aprenderemos algunas funciones avanzadas para optimizar el rendimiento de Spark, para imputar valores faltantes o a crear funciones definidas por el usuario (UDF)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install spark-related dependencies\n",
        "!wget -q  https://apache.osuosl.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAvJUSeiy1-G",
        "outputId": "c00f3ca5-d332-48f7-9409-da75053e793a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "guI6zKboymnn"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AJHnAYm4ymno"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import broadcast\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AbOLspYymno"
      },
      "source": [
        "### Crea la sesión de SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Na7Qb23fymno"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEEYc6vvymno"
      },
      "source": [
        "### Crear el DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CXoss716ymno",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe7879e7-441f-439b-b7b3-e4122719b222"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string, dept: string, salary: bigint]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
        "    (2, \"BBB\", \"dept1\", 1100),\n",
        "    (3, \"CCC\", \"dept1\", 3000),\n",
        "    (4, \"DDD\", \"dept1\", 5500),\n",
        "    (5, \"EEE\", \"dept2\", 8000),\n",
        "    (6, \"FFF\", \"dept2\", 9200),\n",
        "    (7, \"GGG\", \"dept3\", 1100),\n",
        "    (None, None, None, 5500),\n",
        "    (9, \"III\", None, 3500),\n",
        "    (10, None, \"dept5\", 2500)]\n",
        "\n",
        "dept = [(\"dept1\", \"Department - 1\"),\n",
        "        (\"dept2\", \"Department - 2\"),\n",
        "        (\"dept3\", \"Department - 3\"),\n",
        "        (\"dept4\", \"Department - 4\")\n",
        "       ]\n",
        "\n",
        "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
        "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"])\n",
        "\n",
        "# Create Temp Tables\n",
        "df.createOrReplaceTempView(\"empdf\")\n",
        "deptdf.createOrReplaceTempView(\"deptdf\")\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuD-OiRGymno"
      },
      "source": [
        "#  Expresiones SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpcC8N4Tymno"
      },
      "source": [
        "También podemos usar la expresión SQL para la manipulación de datos. Tenemos la función **expr** y también una variante de un método de selección como **selectExpr** para la evaluación de expresiones SQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tICFpNm8ymnp",
        "outputId": "1db966e7-ae95-4bfb-f946-e5dccef2209a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-----+------+------------+\n",
            "|  id|name| dept|salary|salary_level|\n",
            "+----+----+-----+------+------------+\n",
            "|   1| AAA|dept1|  1000|  low_salary|\n",
            "|   2| BBB|dept1|  1100|  low_salary|\n",
            "|   3| CCC|dept1|  3000|  mid_salary|\n",
            "|   4| DDD|dept1|  5500| high_salary|\n",
            "|   5| EEE|dept2|  8000| high_salary|\n",
            "|   6| FFF|dept2|  9200| high_salary|\n",
            "|   7| GGG|dept3|  1100|  low_salary|\n",
            "|NULL|NULL| NULL|  5500| high_salary|\n",
            "|   9| III| NULL|  3500|  mid_salary|\n",
            "|  10|NULL|dept5|  2500|  mid_salary|\n",
            "+----+----+-----+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "cond = \"\"\"case when salary > 5000 then 'high_salary'\n",
        "               else case when salary > 2000 then 'mid_salary'\n",
        "                    else case when salary > 0 then 'low_salary'\n",
        "                         else 'invalid_salary'\n",
        "                              end\n",
        "                         end\n",
        "                end as salary_level\"\"\"\n",
        "\n",
        "#agregamos una nueva columna con withColumn\n",
        "newdf = df.withColumn(\"salary_level\", expr(cond))\n",
        "newdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMQvVvZaymnp"
      },
      "source": [
        "### Usando la función selectExpr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrVd7qppymnp",
        "outputId": "39ff80b5-b56b-4de4-98a3-ac38224222a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-----+------+------------+\n",
            "|  id|name| dept|salary|salary_level|\n",
            "+----+----+-----+------+------------+\n",
            "|   1| AAA|dept1|  1000|  low_salary|\n",
            "|   2| BBB|dept1|  1100|  low_salary|\n",
            "|   3| CCC|dept1|  3000|  mid_salary|\n",
            "|   4| DDD|dept1|  5500| high_salary|\n",
            "|   5| EEE|dept2|  8000| high_salary|\n",
            "|   6| FFF|dept2|  9200| high_salary|\n",
            "|   7| GGG|dept3|  1100|  low_salary|\n",
            "|NULL|NULL| NULL|  5500| high_salary|\n",
            "|   9| III| NULL|  3500|  mid_salary|\n",
            "|  10|NULL|dept5|  2500|  mid_salary|\n",
            "+----+----+-----+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "newdf = df.selectExpr(\"*\", cond)\n",
        "newdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdBm3XtYymnq"
      },
      "source": [
        "### Funciones definidas por el usuario (UDF)\n",
        "A menudo necesitamos escribir la función en función de nuestro requisito muy específico. Aquí podemos aprovechar las udfs. Podemos escribir nuestras propias funciones en un lenguaje como python y registrar la función como udf, luego podemos usar la función para operaciones de DataFrame.\n",
        "\n",
        "* Función de Python para encontrar el nivel_salario para un salario dado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwZquHrPymnq"
      },
      "outputs": [],
      "source": [
        "def detSalary_Level(sal):\n",
        "    level = None\n",
        "\n",
        "    if(sal > 5000):\n",
        "        level = 'high_salary'\n",
        "    elif(sal > 2000):\n",
        "        level = 'mid_salary'\n",
        "    elif(sal > 0):\n",
        "        level = 'low_salary'\n",
        "    else:\n",
        "        level = 'invalid_salary'\n",
        "    return level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29JJzxMymnq"
      },
      "source": [
        "* Luego registre la función \"detSalary_Level\" como UDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aurtlp89ymnq"
      },
      "outputs": [],
      "source": [
        "sal_level = udf(detSalary_Level, StringType())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsLdoEOBymnq"
      },
      "source": [
        "* Aplicar función para determinar el salario_level para un salario dado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfZ5Szf8ymnq",
        "outputId": "1d941a2f-3a63-4c86-e111-1b7e4f8d0102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-----+------+------------+\n",
            "|  id|name| dept|salary|salary_level|\n",
            "+----+----+-----+------+------------+\n",
            "|   1| AAA|dept1|  1000|  low_salary|\n",
            "|   2| BBB|dept1|  1100|  low_salary|\n",
            "|   3| CCC|dept1|  3000|  mid_salary|\n",
            "|   4| DDD|dept1|  5500| high_salary|\n",
            "|   5| EEE|dept2|  8000| high_salary|\n",
            "|   6| FFF|dept2|  9200| high_salary|\n",
            "|   7| GGG|dept3|  1100|  low_salary|\n",
            "|NULL|NULL| NULL|  5500| high_salary|\n",
            "|   9| III| NULL|  3500|  mid_salary|\n",
            "|  10|NULL|dept5|  2500|  mid_salary|\n",
            "+----+----+-----+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "newdf = df.withColumn(\"salary_level\", sal_level(\"salary\"))\n",
        "newdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trabajando con valores NULL\n",
        "\n",
        "Los valores NULL siempre son difíciles de manejar independientemente del Framework o lenguaje que usemos. Aquí en Spark tenemos pocas funciones específicas para lidiar con valores NULL.\n",
        "\n",
        "**isNull**()\n",
        "\n",
        "Esta función nos ayudará a encontrar los valores nulos para cualquier columna dada. Por ejemplo si necesitamos encontrar las columnas donde las columnas id contienen los valores nulos."
      ],
      "metadata": {
        "id": "9NM1Qa0J1-OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newdf = df.filter(df[\"dept\"].isNull())\n",
        "newdf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGzeGSii2Vjk",
        "outputId": "f085761c-035a-4c6e-e2cc-4efa30de9975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+------+\n",
            "|  id|name|dept|salary|\n",
            "+----+----+----+------+\n",
            "|NULL|NULL|NULL|  5500|\n",
            "|   9| III|NULL|  3500|\n",
            "+----+----+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**isNotNull**()\n",
        "\n",
        "\n",
        "Esta función funciona de manera opuesta a la función isNull () y devolverá todos los valores no nulos para una función en particular."
      ],
      "metadata": {
        "id": "cI8btU692YD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newdf = df.filter(df[\"dept\"].isNotNull())\n",
        "newdf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cue3oKgL2k3A",
        "outputId": "93fb4299-af7c-4e3b-9846-df5e766709ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  3| CCC|dept1|  3000|\n",
            "|  4| DDD|dept1|  5500|\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  9200|\n",
            "|  7| GGG|dept3|  1100|\n",
            "| 10|NULL|dept5|  2500|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**fillna**()\n",
        "\n",
        "Esta función nos ayudará a reemplazar los valores nulos."
      ],
      "metadata": {
        "id": "72Zzdu1a2swc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace -1 where the salary is null.\n",
        "newdf = df.fillna(\"INVALID\", [\"dept\"])\n",
        "newdf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3237g1T2wDI",
        "outputId": "34d27fbf-6511-4454-d55e-258971120c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-------+------+\n",
            "|  id|name|   dept|salary|\n",
            "+----+----+-------+------+\n",
            "|   1| AAA|  dept1|  1000|\n",
            "|   2| BBB|  dept1|  1100|\n",
            "|   3| CCC|  dept1|  3000|\n",
            "|   4| DDD|  dept1|  5500|\n",
            "|   5| EEE|  dept2|  8000|\n",
            "|   6| FFF|  dept2|  9200|\n",
            "|   7| GGG|  dept3|  1100|\n",
            "|NULL|NULL|INVALID|  5500|\n",
            "|   9| III|INVALID|  3500|\n",
            "|  10|NULL|  dept5|  2500|\n",
            "+----+----+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dropna**()\n",
        "\n",
        "Esta función nos ayudará a eliminar las filas con valores nulos."
      ],
      "metadata": {
        "id": "QXFik0ni2zAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newdf = df.dropna()\n",
        "newdf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBQ1GHxl23ra",
        "outputId": "5a0bec21-5780-4df3-9999-5379b7788381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  3| CCC|dept1|  3000|\n",
            "|  4| DDD|dept1|  5500|\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  9200|\n",
            "|  7| GGG|dept3|  1100|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}