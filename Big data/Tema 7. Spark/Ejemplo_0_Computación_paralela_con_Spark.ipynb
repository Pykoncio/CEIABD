{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I7p_uqUiIb3"
      },
      "source": [
        "# Inicialización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri7MiwC8fOqi"
      },
      "source": [
        "En primer lugar instalamos y configuramos todas las dependencias de Spark para Python. De esta forma enlazaremos nuestro entorno con el servidor de Spark. Además configuraremos el entorno Spark con las variables que sean necesarias.\n",
        "\n",
        "\n",
        "\n",
        "**NOTA:**\n",
        "\n",
        "**La última versión de PySpark es la 3.5.4 [link](https://pypi.org/project/pyspark/#history)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWM4ntmGfLSE",
        "outputId": "eb60accb-284b-408c-f631-cb182d40a680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in c:\\users\\alfre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\alfre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Install spark-related dependencies\n",
        "# !wget -q  https://apache.osuosl.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "# !tar xf spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"D:\\CEIABD\\spark-3.5.4-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dq4j8GfmSRY"
      },
      "source": [
        "Vamos a verificar que el entorno Spark está bien creado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4pMTURnTgoIe",
        "outputId": "64a85901-f340-437b-b2d5-cbc7b17b5207"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'D:\\\\CEIABD\\\\spark-3.5.4-bin-hadoop3'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.environ[\"SPARK_HOME\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCyZxBpShBiP"
      },
      "source": [
        "Vamos a iniciar uan sesión de spark simple para testear nuestra instalación\n",
        "\n",
        "1. Ejecutamos findspark.init() para hacer que pyspark sea importable como una biblioteca normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alfre\\AppData\\Local\\Programs\\Python\\Python311;c:\\Users\\alfre\\AppData\\Roaming\\Python\\Python311\\Scripts;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files\\Java\\jdk-17\\bin;C:\\Program Files\\nodejs\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\WireGuard\\;C:\\Program Files\\Amazon\\AWSCLIV2\\;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Program Files\\Exiftool;C:\\Users\\alfre\\AppData\\Local\\Programs\\oh-my-posh\\bin\\;C:\\Users\\alfre\\.cargo\\bin;C:\\Program Files\\MySQL\\MySQL Shell 8.0\\bin\\;C:\\Users\\alfre\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\alfre\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\alfre\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\alfre\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\msys64\\usr\\bin;C:\\Program Files\\JetBrains\\IntelliJ IDEA Community Edition 2023.1.1\\bin;;C:\\Users\\alfre\\AppData\\Local\\GitHubDesktop\\bin;C:\\Users\\alfre\\AppData\\Roaming\\npm;C:\\Program Files\\PostgreSQL\\16\\bin;C:\\Program Files\\apache\\maven-3.9.5\\bin;C:\\Users\\alfre\\AppData\\Local\\Programs\\oh-my-posh\\bin;C:\\Program Files\\Exiftool;;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files\\Java\\jdk-17\\bin;C:\\Program Files\\nodejs\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\WireGuard\\;C:\\Program Files\\Amazon\\AWSCLIV2\\;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Program Files\\Exiftool;C:\\Users\\alfre\\AppData\\Local\\Programs\\oh-my-posh\\bin\\;C:\\Users\\alfre\\.cargo\\bin;C:\\Program Files\\MySQL\\MySQL Shell 8.0\\bin\\;C:\\Users\\alfre\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\alfre\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\alfre\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\alfre\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\msys64\\usr\\bin;C:\\Program Files\\JetBrains\\IntelliJ IDEA Community Edition 2023.1.1\\bin;;C:\\Users\\alfre\\AppData\\Local\\GitHubDesktop\\bin;C:\\Users\\alfre\\AppData\\Roaming\\npm;C:\\Program Files\\PostgreSQL\\16\\bin;C:\\Program Files\\apache\\maven-3.9.5\\bin;C:\\Users\\alfre\\AppData\\Local\\Programs\\oh-my-posh\\bin;C:\\Program Files\\Exiftool;\n"
          ]
        }
      ],
      "source": [
        "echo %PATH%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y7nrYBCBhBOh"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LZqWpiYnpGt"
      },
      "source": [
        "2. Crear un contexto Spark para ejecutar la aplicación\n",
        "\n",
        "> NOTA: Un SparkContext representa la conexión al cluster de Spark, y puede utilizarse para crear RDDs y otros elementos. **Sólo puede haber un SparkContext activo**. Se debe detener (*stop()*) el SparkContext activo antes de crear uno nuevo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8E2ccfuQWsug"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "jMbWlE-0nbJl",
        "outputId": "fd412803-8eb1-440e-a0ee-686136a9990d"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m conf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.appName\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# create the context\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mpyspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      7\u001b[0m sc\n",
            "File \u001b[1;32mD:\\CEIABD\\spark-3.5.4-bin-hadoop3\\python\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
            "File \u001b[1;32mD:\\CEIABD\\spark-3.5.4-bin-hadoop3\\python\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
            "File \u001b[1;32mD:\\CEIABD\\spark-3.5.4-bin-hadoop3\\python\\pyspark\\java_gateway.py:104\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 104\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "conf = SparkConf()\n",
        "conf.set(\"spark.ui.port\", \"4050\")\n",
        "conf.set(\"spark.appName\", \"Pi\")\n",
        "# create the context\n",
        "sc = pyspark.SparkContext()\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qILlcLo6oFBk"
      },
      "source": [
        "# Paralelizar el cálculo con parallelize:\n",
        "Este método se utiliza para distribuir la colección de elementos del mismo tipo (datos u operaciones) para poder funcionar en paralelo.\n",
        "\n",
        "sc.parallelize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_poGTJ9hndn9"
      },
      "source": [
        "# Particiones en Spark\n",
        "La partición significa que los datos completos no están presentes en un solo lugar. Se divide en varios fragmentos y estos fragmentos se colocan en diferentes nodos.\n",
        "\n",
        "Si tiene una partición, Spark solo tendrá un paralelismo de una, incluso si tiene miles de ejecutores. Además, si tiene muchas particiones pero solo un ejecutor, Spark seguirá teniendo solo un paralelismo de uno porque solo hay un recurso de cálculo.\n",
        "\n",
        "En Spark, las API de nivel inferior nos permiten definir la cantidad de particiones.\n",
        "\n",
        "Tomemos un ejemplo sencillo para entender cómo la partición nos ayuda a dar resultados más rápidos. Crearemos una lista de 20 millones de números aleatorios entre 10 y 1000 y contaremos los números mayores a 200.\n",
        "\n",
        "Veamos qué tan rápido podemos hacer esto con una sola partición:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-HgJFtDlvUY",
        "outputId": "c9a14ca0-9e04-4bae-c7d4-a4df1f270c05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "from random import randint\n",
        "\n",
        "# create a list of random numbers between 10 to 1000\n",
        "my_large_list = [randint(10,1000) for x in range(0,20000000)]\n",
        "\n",
        "# create one partition of the list\n",
        "my_large_list_one_partition = sc.parallelize(my_large_list,numSlices=1)\n",
        "\n",
        "# check number of partitions\n",
        "print(my_large_list_one_partition.getNumPartitions())\n",
        "# >> 1\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_one_partition = my_large_list_one_partition.filter(lambda x : x >= 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r70aTvYEnFmw",
        "outputId": "fa21856b-1211-439e-e0ab-5b85a12129bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16166223\n",
            "CPU times: user 78.6 ms, sys: 11.4 ms, total: 90 ms\n",
            "Wall time: 13.2 s\n"
          ]
        }
      ],
      "source": [
        "# to calculate the time taken to execute the following command\n",
        "%%time\n",
        "\n",
        "# count the number of elements in filtered list\n",
        "print(my_large_list_one_partition.count())\n",
        "# >> 16162207"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXX4YFqxoY6_"
      },
      "source": [
        "Ahora, aumentemos el número de particiones a 5 y verifiquemos si obtenemos alguna mejora en el tiempo de ejecución:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_jAEEHHohZq"
      },
      "outputs": [],
      "source": [
        "# create five partitions of the list\n",
        "my_large_list_with_five_partition = sc.parallelize(my_large_list, numSlices=5)\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_with_five_partition = my_large_list_with_five_partition.filter(lambda x : x >= 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y-2e3Btoo9B",
        "outputId": "eb81b187-97a3-4fb0-87ee-0275973f4260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16166223\n",
            "CPU times: user 57.5 ms, sys: 5.99 ms, total: 63.5 ms\n",
            "Wall time: 9.29 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# count the number of elements in the filtered list\n",
        "print(my_large_list_with_five_partition.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBUPnpdIoWRu"
      },
      "source": [
        "Paramos el SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB9mF_zJoVal"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
