{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I7p_uqUiIb3"
      },
      "source": [
        "# Inicialización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri7MiwC8fOqi"
      },
      "source": [
        "En primer lugar instalamos y configuramos todas las dependencias de Spark para Python. De esta forma enlazaremos nuestro entorno con el servidor de Spark. Además configuraremos el entorno Spark con las variables que sean necesarias.\n",
        "\n",
        "\n",
        "\n",
        "**NOTA:**\n",
        "\n",
        "**La última versión de PySpark es la 3.5.4 [link](https://pypi.org/project/pyspark/#history)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWM4ntmGfLSE",
        "outputId": "eb60accb-284b-408c-f631-cb182d40a680"
      },
      "source": [
        "# Install spark-related dependencies\n",
        "!wget -q  https://apache.osuosl.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dq4j8GfmSRY"
      },
      "source": [
        "Vamos a verificar que el entorno Spark está bien creado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4pMTURnTgoIe",
        "outputId": "64a85901-f340-437b-b2d5-cbc7b17b5207"
      },
      "source": [
        "os.environ[\"SPARK_HOME\"]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.5.4-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCyZxBpShBiP"
      },
      "source": [
        "Vamos a iniciar uan sesión de spark simple para testear nuestra instalación\n",
        "\n",
        "1. Ejecutamos findspark.init() para hacer que pyspark sea importable como una biblioteca normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7nrYBCBhBOh"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LZqWpiYnpGt"
      },
      "source": [
        "2. Crear un contexto Spark para ejecutar la aplicación\n",
        "\n",
        "> NOTA: Un SparkContext representa la conexión al cluster de Spark, y puede utilizarse para crear RDDs y otros elementos. **Sólo puede haber un SparkContext activo**. Se debe detener (*stop()*) el SparkContext activo antes de crear uno nuevo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E2ccfuQWsug"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMbWlE-0nbJl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "fd412803-8eb1-440e-a0ee-686136a9990d"
      },
      "source": [
        "conf = SparkConf()\n",
        "conf.set(\"spark.ui.port\", \"4050\")\n",
        "conf.set(\"spark.appName\", \"Pi\")\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b15fc627b3f0:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paralelizar el cálculo con parallelize:\n",
        "Este método se utiliza para distribuir la colección de elementos del mismo tipo (datos u operaciones) para poder funcionar en paralelo.\n",
        "\n",
        "sc.parallelize()"
      ],
      "metadata": {
        "id": "qILlcLo6oFBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Particiones en Spark\n",
        "La partición significa que los datos completos no están presentes en un solo lugar. Se divide en varios fragmentos y estos fragmentos se colocan en diferentes nodos.\n",
        "\n",
        "Si tiene una partición, Spark solo tendrá un paralelismo de una, incluso si tiene miles de ejecutores. Además, si tiene muchas particiones pero solo un ejecutor, Spark seguirá teniendo solo un paralelismo de uno porque solo hay un recurso de cálculo.\n",
        "\n",
        "En Spark, las API de nivel inferior nos permiten definir la cantidad de particiones.\n",
        "\n",
        "Tomemos un ejemplo sencillo para entender cómo la partición nos ayuda a dar resultados más rápidos. Crearemos una lista de 20 millones de números aleatorios entre 10 y 1000 y contaremos los números mayores a 200.\n",
        "\n",
        "Veamos qué tan rápido podemos hacer esto con una sola partición:"
      ],
      "metadata": {
        "id": "_poGTJ9hndn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "# create a list of random numbers between 10 to 1000\n",
        "my_large_list = [randint(10,1000) for x in range(0,20000000)]\n",
        "\n",
        "# create one partition of the list\n",
        "my_large_list_one_partition = sc.parallelize(my_large_list,numSlices=1)\n",
        "\n",
        "# check number of partitions\n",
        "print(my_large_list_one_partition.getNumPartitions())\n",
        "# >> 1\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_one_partition = my_large_list_one_partition.filter(lambda x : x >= 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-HgJFtDlvUY",
        "outputId": "c9a14ca0-9e04-4bae-c7d4-a4df1f270c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to calculate the time taken to execute the following command\n",
        "%%time\n",
        "\n",
        "# count the number of elements in filtered list\n",
        "print(my_large_list_one_partition.count())\n",
        "# >> 16162207"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r70aTvYEnFmw",
        "outputId": "fa21856b-1211-439e-e0ab-5b85a12129bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16166223\n",
            "CPU times: user 78.6 ms, sys: 11.4 ms, total: 90 ms\n",
            "Wall time: 13.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, aumentemos el número de particiones a 5 y verifiquemos si obtenemos alguna mejora en el tiempo de ejecución:"
      ],
      "metadata": {
        "id": "mXX4YFqxoY6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create five partitions of the list\n",
        "my_large_list_with_five_partition = sc.parallelize(my_large_list, numSlices=5)\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_with_five_partition = my_large_list_with_five_partition.filter(lambda x : x >= 200)"
      ],
      "metadata": {
        "id": "X_jAEEHHohZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# count the number of elements in the filtered list\n",
        "print(my_large_list_with_five_partition.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y-2e3Btoo9B",
        "outputId": "eb81b187-97a3-4fb0-87ee-0275973f4260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16166223\n",
            "CPU times: user 57.5 ms, sys: 5.99 ms, total: 63.5 ms\n",
            "Wall time: 9.29 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBUPnpdIoWRu"
      },
      "source": [
        "Paramos el SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB9mF_zJoVal"
      },
      "source": [
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}