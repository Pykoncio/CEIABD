{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axHd8Uoavrut"
      },
      "source": [
        "# Conociendo los RDDs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb3NmQZXvruv"
      },
      "source": [
        "En este notebook trabajaremos con los RDDs, que forman parte del Spark Core.La implementación de Spark Core es un **RDD (Resilient Distributed Dataset)** que es una colección de datos distribuidos en diferentes nodos del clúster que se procesan en paralelo.\n",
        "\n",
        "Utilizaremos la API de PySpark, pero los conceptos aplican por igual a todas las APIs (Scala, R, etc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYD-Qewivruv"
      },
      "source": [
        "### Inicialización de Spark en Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install spark-related dependencies\n",
        "!wget -q  https://apache.osuosl.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "165ZsCTCwbBw",
        "outputId": "ef943c12-06ac-4ed9-f1bb-7f00b685046a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7l9vgGAyvruv"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init() #para inicializar\n",
        "\n",
        "import pandas as pd\n",
        "import pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bafMqNYivruw"
      },
      "source": [
        "### Crear el SparkSession y el SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4AG01NLxvruw"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName('PySpark_training')\\\n",
        "        .getOrCreate() #devuelve una sesion existente sino existe la crea\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**local[*]**\n",
        "\n",
        "* \"local\" significa que Spark se ejecutará en modo local, en tu máquina actual, en lugar de en un clúster de máquinas. Esto es útil para desarrollo y pruebas.\n",
        "\n",
        "* El [*] le dice a Spark que use todos los núcleos de CPU disponibles en tu máquina. El asterisco actúa como un comodín, determinando dinámicamente el número de núcleos a utilizar."
      ],
      "metadata": {
        "id": "eNmbMrg8woJR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExFa44Jzvrux"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.getOrCreate() #genera lo mismo que antes solo que con los valores por default\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhVsIC5vvrux"
      },
      "source": [
        "### Crear un RDD de una colección"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrcYLticvrux",
        "outputId": "73ec9a67-d382-4a87-a672-23bf11e1c22d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "num = [1,2,3,4,5]\n",
        "\n",
        "num_rdd = sc.parallelize(num) #función para paralelizar\n",
        "num_rdd.collect() #con collect, recogemos toda la lista de números"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "busHkamEvrux"
      },
      "source": [
        "# Transformaciones\n",
        "* Como sabemos, las Transformaciones son de naturaleza perezosa y no se ejecutarán hasta que se ejecute una Acción sobre ellas.\n",
        "* Intentemos comprender las distintas transformaciones disponibles.\n",
        "\n",
        "Para mas informacion, puedes apoyarte del siguiente link:\n",
        "* https://keepcoding.io/blog/transformaciones-y-acciones-en-spark/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpSqfxuTvruy"
      },
      "source": [
        "### map\n",
        "* Esto mapeará su entrada a alguna salida basada en la función especificada en la función"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwJzYb7_vruy",
        "outputId": "7e0ddba9-c128-49b2-b0f7-594385c499ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 6, 8, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "double_rdd = num_rdd.map(lambda x : x * 2)\n",
        "double_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**collect()** toma todos los datos distribuidos en el clúster Spark (o en tu máquina local si estás trabajando en modo local) y los reúne en una sola lista en tu programa, donde puedes acceder a ellos y trabajar con ellos directamente.\n",
        "\n",
        "**Importante:**\n",
        "\n",
        "* collect() es una acción, lo que significa que desencadena la ejecución de todas las transformaciones previas que se hayan aplicado al RDD.\n",
        "* Se debe usar con precaución en RDDs grandes, ya que traer todos los datos al programa principal puede consumir mucha memoria y tiempo. Es preferible usar otras acciones como **take()** para obtener una muestra de los datos si solo necesitas una parte de ellos."
      ],
      "metadata": {
        "id": "ARQXEXv_yZGQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzkEaJlevruy"
      },
      "source": [
        "### filtro\n",
        "* Para filtrar los datos en función de una determinada condición. Intentemos encontrar los números pares de num_rdd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8nZKVjMjvruy",
        "outputId": "c67642de-5a07-4a7d-e476-53797b18b5fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "even_rdd = num_rdd.filter(lambda x : x % 2 == 0) #filtro por los elementos pares del rdd\n",
        "even_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRJYm-LVvruy"
      },
      "source": [
        "### distinct\n",
        "* Esto devolverá elementos distintos de un RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "V6J2ePcIvruy",
        "outputId": "7df921e1-9d27-4b9a-cf10-6ddfb7cee2af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 12, 11, 13]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "rdd1 = sc.parallelize([10, 11, 11, 13, 11, 10, 12])\n",
        "\n",
        "# la función distinct sirve para eliminar elementos duplicados de un RDD.\n",
        "dist_rdd = rdd1.distinct()\n",
        "\n",
        "#dist_rdd = dist_rdd.sortBy(lambda x: x) #por si queremos los elementos ordenados\n",
        "dist_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4V85QMPvruy"
      },
      "source": [
        "### reduceByKey\n",
        "* **Agrupación por clave**: reduceByKey agrupa todos los elementos del RDD que tienen la misma clave. En este caso, agrupará los elementos con clave \"a\", los elementos con clave \"b\" y los elementos con clave \"c\".\n",
        "\n",
        "* **Reducción de valores**: Después de la agrupación, reduceByKey aplica la función \"lambda x, y : x + y\" a los valores asociados a cada clave. Esta función simplemente suma los dos valores (x e y) que se le pasan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mxzf_XBSvruy",
        "outputId": "7faa7201-29df-40ea-d3bb-e9e2fe6f6217",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('a', 13)\n",
            "('b', 4)\n",
            "('c', 7)\n"
          ]
        }
      ],
      "source": [
        "pairs = [ (\"a\", 8), (\"b\", 3), (\"c\", 3), (\"a\", 5), (\"b\", 1), (\"c\", 4)]\n",
        "pair_rdd = sc.parallelize(pairs)\n",
        "\n",
        "output = pair_rdd.reduceByKey(lambda x, y : x + y)\n",
        "\n",
        "result = output.sortByKey().collect()\n",
        "print(*result, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlZqLOb5vruz"
      },
      "source": [
        "### sortByKey\n",
        "* Esta función realizará la clasificación en un par (clave, valor) RDD basado en las claves. De forma predeterminada, la clasificación se realizará en orden ascendente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "QzGtFUYVvruz",
        "outputId": "5a8b3317-e2d2-4c25-ede1-12987235b5b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('a', 5)\n",
            "('b', 3)\n",
            "('c', 2)\n",
            "('d', 7)\n"
          ]
        }
      ],
      "source": [
        "pairs = [ (\"a\", 5), (\"d\", 7), (\"c\", 2), (\"b\", 3)]\n",
        "raw_rdd = sc.parallelize(pairs)\n",
        "\n",
        "sortkey_rdd = raw_rdd.sortByKey() #ascending=True\n",
        "#sortkey_rdd = raw_rdd.sortByKey(ascending=False)\n",
        "result = sortkey_rdd.collect()\n",
        "print(*result,sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQSRE42nvruz"
      },
      "source": [
        "# Acciones\n",
        "\n",
        "* Las acciones son operaciones en RDD que se ejecutan inmediatamente. Mientras que las transformaciones devuelven otro RDD, las acciones devuelven estructuras de datos nativas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5PDTU4tvruz"
      },
      "source": [
        "### count\n",
        "* Esto contará el número de elementos en el RDD dado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nFAvBy3gvruz",
        "outputId": "8c678248-d544-4cec-c4ab-ac57a260edc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "num = sc.parallelize([1,2,4,5,2])\n",
        "num.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpClfbTxvruz"
      },
      "source": [
        "### first\n",
        "* Esto devolverá el primer elemento del RDD dado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pv7Wo6gyvruz",
        "outputId": "b29f4ebb-eb08-484a-d42f-e6032b71e975",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "num.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tetkUm2Lvru0"
      },
      "source": [
        "### Collect\n",
        "* Esto devolverá todos los elementos para el RDD dado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRKepWbYvru0",
        "outputId": "3adcceaf-b12f-42b0-dd96-532f92b2010e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 2, 4, 5, 2]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxIXKGwHvru0"
      },
      "source": [
        "### Take\n",
        "* Esto devolverá el número de elementos especificados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pDrzv_I4vru0",
        "outputId": "91cba177-6b74-415a-f98a-05c71a770ef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "num.take(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "5qd7CfWk1x_T"
      },
      "execution_count": 38,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}